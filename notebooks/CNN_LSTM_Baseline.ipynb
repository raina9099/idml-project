{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ayQIkU7Em2v"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoWvbWIZTPUv",
        "outputId": "31dee983-4e51-4ca5-95fb-01e5ac1cd915"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "# !pip install torchsummaryX\n",
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWEgeL-KIZtq",
        "outputId": "d6bba5be-2cf2-4f6a-c298-f559957f55df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.8)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.0.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rsitW6SEp-9"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZGf0Jj1CRtr",
        "outputId": "57e34a36-2c4b-4445-a388-1df3e1031abf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from datasets import load_dataset\n",
        "import editdistance\n",
        "# from torchsummary import summary\n",
        "from torchinfo import summary\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MccBiCSzI3DD"
      },
      "source": [
        "# Hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fJj4IjrhI46X"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"lr\"         : 0.002,\n",
        "    \"epochs\"     : 20,\n",
        "    \"batch_size\" : 64,\n",
        "    'img_height': 128,\n",
        "    'img_width': 1000,\n",
        "    'lstm_num_layers': 4,\n",
        "    'lstm_hidden_size': 256,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssdvA7QULest"
      },
      "source": [
        "#Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmfEAhc-ut0p",
        "outputId": "cb7ee024-ab9e-4f42-a665-92582a25555b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "data": {
            "text/plain": [
              "425"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "NtCFs-fLCRwA"
      },
      "outputs": [],
      "source": [
        "IMAGE_HEIGHT = config['img_height']\n",
        "IMAGE_WIDTH = config['img_width']\n",
        "BATCH_SIZE = config['batch_size']\n",
        "NUM_EPOCHS = config['epochs']\n",
        "LEARNING_RATE = config['lr']\n",
        "\n",
        "# Load the dataset from Hugging Face\n",
        "dataset = load_dataset(\"Teklia/IAM-line\")\n",
        "\n",
        "# Define transforms\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
        "#     transforms.ToTensor(),\n",
        "# ])\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "# Create a character-to-index mapping\n",
        "chars = set(''.join(dataset['train']['text']))\n",
        "char_to_index = {char: idx + 1 for idx, char in enumerate(sorted(chars))}\n",
        "char_to_index['<PAD>'] = 0\n",
        "index_to_char = {idx: char for char, idx in char_to_index.items()}\n",
        "vocab_size = len(char_to_index)\n",
        "\n",
        "# Tokenizer functions\n",
        "def tokenize(text):\n",
        "    return [char_to_index[char] for char in text]\n",
        "\n",
        "def detokenize(indices):\n",
        "    return ''.join([index_to_char[idx] for idx in indices if idx != 0])\n",
        "\n",
        "# Dataset class\n",
        "class IAMLinesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, hf_dataset, split, transform=None):\n",
        "        self.dataset = hf_dataset[split]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        image = item['image']\n",
        "        text = item['text']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # print(\"Before tokenization: \", len(text), \"text: \", text)\n",
        "        target = torch.LongTensor(tokenize(text))\n",
        "\n",
        "        return image, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, targets = zip(*batch)\n",
        "    images = torch.stack(images)\n",
        "\n",
        "    # Pad sequences\n",
        "    targets = pad_sequence(targets, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Create mask for actual lengths\n",
        "    target_lengths = torch.LongTensor([len(t) for t in targets])\n",
        "\n",
        "    # return images, targets, target_lengths\n",
        "    return images, targets\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = IAMLinesDataset(dataset, 'train', transform)\n",
        "val_dataset = IAMLinesDataset(dataset, 'validation', transform)\n",
        "test_dataset = IAMLinesDataset(dataset, 'test', transform)\n",
        "\n",
        "# Create dataloaders with the custom collate_fn\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYjMKwpuLOZ4",
        "outputId": "1539f82d-e2b7-4ae9-aac2-e1617c97ed70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples in train set: 6482\n",
            "Number of samples in test set: 2915\n",
            "Number of samples in val set: 976\n",
            "Batch size: 64\n",
            "Num batches in train_loader: 102\n",
            "Num batches in test_loader: 46\n",
            "Num batches in val_loader: 16\n",
            "Shape of 23-th item in train set:\n",
            "Image: torch.Size([1, 128, 1000]), Label: torch.Size([39])\n",
            "Shape of 23-th item in a batch of train loader:\n",
            "Shape of entire batch of images: torch.Size([64, 1, 128, 1000])\n",
            "Shape of entire batch of labels: torch.Size([64, 65])\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of samples in train set: {len(train_dataset)}\")\n",
        "print(f\"Number of samples in test set: {len(test_dataset)}\")\n",
        "print(f\"Number of samples in val set: {len(val_dataset)}\")\n",
        "\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Num batches in train_loader: {len(train_loader)}\")\n",
        "print(f\"Num batches in test_loader: {len(test_loader)}\")\n",
        "print(f\"Num batches in val_loader: {len(val_loader)}\")\n",
        "\n",
        "i=23\n",
        "print(f\"Shape of {i}-th item in train set:\")\n",
        "print(f\"Image: {train_dataset[i][0].shape}, Label: {train_dataset[i][1].shape}\")\n",
        "\n",
        "print(f\"Shape of {i}-th item in a batch of train loader:\")\n",
        "x = next(iter(train_loader)) # (batch of 32 images, batch of 32 labels)\n",
        "print(f\"Shape of entire batch of images: {x[0].shape}\")\n",
        "print(f\"Shape of entire batch of labels: {x[1].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5L6i68CLhpE"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "i9sVZKEtOzqz"
      },
      "outputs": [],
      "source": [
        "class CNN_LSTM(nn.Module):\n",
        "    def __init__(self, hidden_size, num_layers, num_classes):\n",
        "        super(CNN_LSTM, self).__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3, 3), stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),  # Reduce height and width by 2\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))  # Further reduce height and width by 2\n",
        "        )\n",
        "\n",
        "        # LSTM part\n",
        "        # LSTM input size = out_channels of last Conv2d * img height after all conv layers i.e input_img_width / (2**num_maxpool_layers)\n",
        "        self.lstm = nn.LSTM(input_size=128*32, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(input_size=512*8, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
        "        nn.BatchNorm2d(2 * hidden_size)\n",
        "        self.fc = nn.Linear(2 * hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input x has shape (batch_size, channels, height, width)\n",
        "        # print(\"input: \", x.shape) # (32, 1, 128, 1000]\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Pass through CNN (output will have shape (batch_size, 128, 32, 250))\n",
        "        out = self.cnn(x)\n",
        "        # print(\"cnn output: \", out.shape) # (32, 128, 32, 250)\n",
        "        # Reshape output from CNN to prepare it for LSTM\n",
        "        # New shape: (batch_size, 250, 128*32) where 250 is the reduced width (seq_len)\n",
        "        # print(\"before reshape output: \", out.shape) # (32, 128, 32, 250)\n",
        "        batch_size, cnn_output_channels, cnn_output_height, cnn_output_width = out.shape #(32, 128, 32, 250)\n",
        "        out = out.reshape(batch_size, cnn_output_width, cnn_output_channels * cnn_output_height)\n",
        "        # print(\"after reshape output: \", out.shape)\n",
        "\n",
        "        # Pass through LSTM (input shape (batch_size, seq_len=250, input_size=128*32))\n",
        "        out, _ = self.lstm(out)\n",
        "        # print(\"lstm output: \", out.shape)\n",
        "\n",
        "        out = self.fc(out)\n",
        "        # print(\"fc output: \", out.shape)\n",
        "\n",
        "        out = F.log_softmax(out, dim=2)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bdjQgVKEbcy",
        "outputId": "9101540e-b12c-4a8e-c88f-7d810348391a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "CNN_LSTM                                 [1, 250, 80]              --\n",
              "├─Sequential: 1-1                        [1, 128, 32, 250]         --\n",
              "│    └─Conv2d: 2-1                       [1, 64, 128, 1000]        640\n",
              "│    └─BatchNorm2d: 2-2                  [1, 64, 128, 1000]        128\n",
              "│    └─ReLU: 2-3                         [1, 64, 128, 1000]        --\n",
              "│    └─MaxPool2d: 2-4                    [1, 64, 64, 500]          --\n",
              "│    └─Conv2d: 2-5                       [1, 128, 64, 500]         73,856\n",
              "│    └─BatchNorm2d: 2-6                  [1, 128, 64, 500]         256\n",
              "│    └─ReLU: 2-7                         [1, 128, 64, 500]         --\n",
              "│    └─MaxPool2d: 2-8                    [1, 128, 32, 250]         --\n",
              "├─LSTM: 1-2                              [1, 250, 512]             13,647,872\n",
              "├─Linear: 1-3                            [1, 250, 80]              41,040\n",
              "==========================================================================================\n",
              "Total params: 13,763,792\n",
              "Trainable params: 13,763,792\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 5.86\n",
              "==========================================================================================\n",
              "Input size (MB): 0.51\n",
              "Forward/backward pass size (MB): 197.79\n",
              "Params size (MB): 55.06\n",
              "Estimated Total Size (MB): 253.36\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = CNN_LSTM(hidden_size=config['lstm_hidden_size'], num_layers=config['lstm_num_layers'], num_classes=vocab_size).to(device)\n",
        "batch_size=1\n",
        "input_channels=1\n",
        "img_height=128\n",
        "img_width=1000\n",
        "summary(model, (batch_size, input_channels, img_height, img_width))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2M-RKnlRoXb"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "grLxLMJVDHjO"
      },
      "outputs": [],
      "source": [
        "# Initialize model, loss, and optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "criterion = nn.CTCLoss(blank=0)\n",
        "optimizer =  torch.optim.AdamW(model.parameters(), lr= config['lr'], weight_decay=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "scaler = torch.amp.GradScaler('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "IpYUNZp4RjBa"
      },
      "outputs": [],
      "source": [
        "def ctc_decode_tensor(predicted_indices):\n",
        "    if not isinstance(predicted_indices, torch.Tensor):\n",
        "        predicted_indices = torch.tensor(predicted_indices)\n",
        "\n",
        "    batch_size, seq_length = predicted_indices.shape\n",
        "    non_blank_mask = predicted_indices != 0\n",
        "    diff_mask = torch.cat([torch.ones(batch_size, 1, dtype=torch.bool, device=predicted_indices.device),\n",
        "                           predicted_indices[:, 1:] != predicted_indices[:, :-1]], dim=1)\n",
        "    valid_mask = non_blank_mask & diff_mask\n",
        "    decoded = predicted_indices * valid_mask.long()\n",
        "    decoded_list = [seq[seq != 0].tolist() for seq in decoded]\n",
        "\n",
        "    return decoded_list\n",
        "\n",
        "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         metric[0]                  : metric[1],\n",
        "         'epoch'                    : epoch},\n",
        "         path\n",
        "    )\n",
        "\n",
        "def load_model(path, model, metric= 'valid_acc', optimizer= None, scheduler= None):\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer != None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if scheduler != None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    epoch   = checkpoint['epoch']\n",
        "    metric  = checkpoint[metric]\n",
        "\n",
        "    return [model, optimizer, scheduler, epoch, metric]\n",
        "\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_distance = 0\n",
        "    total_length = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Get the predicted indices\n",
        "            predicted_indices = torch.argmax(outputs, dim=2) #(batchsize, seqlen)\n",
        "            decoded_predictions = ctc_decode_tensor(predicted_indices)\n",
        "\n",
        "            # Convert predictions and targets to strings\n",
        "            predicted_strings = [''.join([index_to_char[idx.item() if isinstance(idx, torch.Tensor) else idx] for idx in pred if idx != 0]) for pred in decoded_predictions]\n",
        "            target_strings = [''.join([index_to_char[idx.item() if isinstance(idx, torch.Tensor) else idx] for idx in tgt if idx != 0]) for tgt in targets]\n",
        "\n",
        "            # Compute edit distance\n",
        "            for pred, tgt in zip(predicted_strings, target_strings):\n",
        "                # print(\"Predicted string: \", pred)\n",
        "                # print(\"Target string: \", tgt)\n",
        "                distance = editdistance.eval(pred, tgt)\n",
        "                # print(\"Distance: \", distance)\n",
        "                total_distance += distance\n",
        "                total_length += len(tgt)\n",
        "\n",
        "            del images, targets, predicted_indices, decoded_predictions, predicted_strings, target_strings, outputs, distance\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Compute Character Error Rate (CER)\n",
        "    cer = total_distance / total_length\n",
        "    return cer\n",
        "\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs, learning_rate, device):\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "            # print(\"targets: \", targets.shape) #(32, 60)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # with torch.cuda.amp.autocast():\n",
        "            with torch.amp.autocast('cuda'):\n",
        "            # Forward pass\n",
        "              outputs = model(images)\n",
        "              # print(\"Forward output: \", outputs.shape) #(32, 250, 80)\n",
        "\n",
        "              # Prepare CTC loss inputs\n",
        "              input_lengths = torch.full(size=(outputs.size(0),), fill_value=outputs.size(1), dtype=torch.long)\n",
        "              # print(\"input_lengths: \", input_lengths.shape) #(32)\n",
        "              target_lengths = torch.sum(targets != 0, dim=1)\n",
        "              # print(\"target_lengths: \", target_lengths.shape) #(32)\n",
        "\n",
        "              # Compute loss\n",
        "              loss = criterion(outputs.transpose(0, 1), targets, input_lengths, target_lengths)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            del images, targets, outputs, input_lengths, target_lengths, loss\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (batch_idx + 1))),\n",
        "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "            batch_bar.update()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f'\\nEpoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "        # Validate after each epoch\n",
        "        val_cer = evaluate(model, val_loader, device)\n",
        "        print(f'Validation CER: {val_cer:.4f}')\n",
        "\n",
        "        batch_bar.close()\n",
        "\n",
        "    save_model(model, optimizer, scheduler, ['valid_dist', val_cer], epoch, \"cnn-lstm-model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUykXoWTRsEL",
        "outputId": "713e31a5-9d0a-4b19-d907-0f664abdb20d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100%|██████████| 102/102 [01:08<00:00,  1.78it/s, loss=4.2282, lr=0.002000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [1/20], Average Loss: 4.2282\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation CER: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train:   5%|▍         | 5/102 [00:03<01:04,  1.50it/s, loss=3.2479, lr=0.002000]"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "train(model, train_loader, val_loader, criterion, optimizer, num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE, device=device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "3ayQIkU7Em2v",
        "0rsitW6SEp-9",
        "ssdvA7QULest",
        "OTJGGiZfu43y"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
